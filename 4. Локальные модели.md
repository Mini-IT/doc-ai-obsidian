---
tags:
  - MiniIT
  - ChatGPT
  - Obsidian
---

### Индексирование базы

У плагина есть ещё один режим работы — QA Vault, в котором вся база индексируется с помощью embedding-модели, благодаря чему модель чата возвращает ответ, более релевантный вашим данным.

Опять же, работает это далеко не идеально, и нужно разбираться в том какие модели существуют, для чего подходят, и как их тонко подстраивать. В своей работе я вообще пока отказался от этого режима — обычного чата достаточно, да и работает он зачастую лучше. Но, возможно, кому-то будет интересно это попробовать.

Нам понадобится:
- Запустить локальный сервер (я покажу на примере LM Client, ещё есть популярный вариант — Ollama)
- Загрузить embedding-модель
- Подключить эту модель в Copilot

---

### Установка LM Client
Скачиваем и устанавливаем менеджер локальных моделей отсюда: https://lmstudio.ai

Запускаем и нажимаем на кнопку с лупой. Если панельки слева нет — убедитесь что переключатель не установлен в положение "User":

![[Pasted image 20250115162520.png]]

Вводим в поле поиска "embed", выбираем модель, нажимаем "Download" и ждём пока она скачается:

![[Pasted image 20250115174421.png]]

Затем переходим в раздел "Developer", нажимаем на кнопку "Select model to load", и выбираем нашу модель. После того как она загрузится в память, включаем все переключатели в настройках сервера, и копируем идентификатор:

![[Pasted image 20250115174535.png]]

Больше никаких действий в LM Studio совершать не придётся — программа сама добавится в автозагрузку, а при запросе будет автоматически загружать модель и запускать сервер.

### Подключение модели в Copilot

Открываем настройки Copilot, листаем до раздела "QA Settings" и раскрываем блок "Add Custom Model":

![[Pasted image 20250115163240.png]]

Вставляем в поле ввода идентификатор нашей модели. В выпадушке "Provider" выбираем "lm-studio". Неважно кто разработчик модели, важно каким менеджером моделей мы пользуемся (соответственно, в случае с Ollama нужно будет выбрать её).

Нажимаем "Add Model" (можно предварительно прожать "Verify Connection").

![[Pasted image 20250115174628.png]]

После успешного подключения поднимаемся выше, и устанавливаем нашу модель как модель по умолчанию. В документации написано, что в некоторых случаях также нужно выставлять галочку "CORS", но я не заметил разницы.

![[Pasted image 20250115174659.png]]

Если же подключение не срабатывает, можно посмотреть что происходит в LM Studio. Например, некоторые модели начинают грузиться по многу раз (и сжирают всю оперативку). Ещё можно посмотреть что происходит в логах:

![[Pasted image 20250115174738.png]]

---

> [!warning] Важно!
> После добавления модели обязательно перезагрузите Obsidian. Это совершенно неочевидный момент, но скорее всего без перезагрузки дальше ничего работать не будет.

---

### Режим QA Vault

Наконец, можем посмотреть как это работает. Для примера закинем в нашу базу документацию для Snipe: https://github.com/Mini-IT/doc-snipe-client-ru

В панели плагина меняем режим с "chat" на "vault QA". В качестве модели  чата оставляем ChatGPT, embedding-модель можно поменять только в настройках, в панели плагина она не отображается. Должно получиться так:

![[Pasted image 20250115170316.png]]

Прожмём кнопку с иконкой паззлика для индексации нашего хранилища. Лучше делать это каждый раз после добавления в базу новых данных. Сверху справа должно появиться сообщение о том, что индексация прошла успешна (если не работает — убедитесь, что перезагрузили Obsidian после добавления модели):

![[Pasted image 20250115175208.png]]

Попробуем задать вопрос, ответ на который есть в нашей базе, при этом не будем указывать в какой именно заметке содержатся нужные данные.

Видим что в чате теперь выводится список источников, но при этом ответ весьма расплывчатый, а в список не попала заметка в которой содержится точная информация:

![[Pasted image 20250115181656.png]]

Возвращаемся в настройки Copilot и ищем в разделе "QA Settings" настройки "Max Sources" и "Requests per second" — увеличим их чтобы модель работала лучше:

![[Pasted image 20250115181827.png]]

Повторяем вопрос, и видим что ответ стал точным, а список источников увеличился, и теперь содержит нужную заметку:

![[Pasted image 20250115181607.png]]

---

### Локальная модель для чата

Кроме того, в LM Studio можно загрузить и модели для чата, чтобы вообще отказаться от ChatGPT, и выполнять все вычисления на своей машине — бесплатно и без интернета.

Какую именно модель выбрать — вопрос для отдельного исследования. Разработчик плагина рекомендует начать с llama3b или mathstral7b, но даже у них есть множество версий.

Короче, ищем и скачиваем подходящую модель:

![[Pasted image 20250116141426.png]]

Затем проделываем ту же операцию что и для embedding-модели. Загружаем её в память, включаем все тогглы и копируем идентификатор:

![[Pasted image 20250116141648.png]]

Подключаем модель в плагин, но уже не в разделе "QA Settings", а в самом верху (будьте внимательны, эти разделы легко перепутать):

![[Pasted image 20250116141817.png]]

Перезагружаем Obsidian, и проверяем что новая модель появилась в списке моделей чата:

![[Pasted image 20250116142006.png]]

Эту модель можно выбирать как в режиме "Chat" так и в режиме "Vault QA". Кроме того, с ней можно общаться и через LM Studio:

![[Pasted image 20250116142323.png]]

Что ж, теперь вы знаете как подключать любые локальные модели.
Вы великолепны!

---


### [[5. Смотри также]]
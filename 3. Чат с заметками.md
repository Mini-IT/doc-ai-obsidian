



# Контекст

Теперь добавим в нашу базу какие-то данные. Например, мы делаем настольную ролевую игру. Создаём заметку описанием персонажей:

![](img/20250110173902.png)

Если задать чату вопрос "кто сильнее — гриф или сурикат?" не указывая наш контекст, он справедливо сообщит что это слишком разные животные для сравнения:

![](img/20250110174002.png)

Чтобы добавить ему контекст, в тексте промпта нужно указать название заметки в двойных квадратных скобках:

![](img/20250110174330.png)

Усложним задачу. Заводим ещё одну заметку с табличкой ассортимента товаров в магазине, и указываем обе заметки в промпте. Работает:

![](img/20250110180543.png)




# Промпты

Теперь добавим в нашу базу промпты, чтобы не вводить их каждый раз вручную. Создаём папку prompts, а внутри неё заметку "Проверка", в которой описываем правила проверки навыка для нашей игры:

![](img/20250110182038.png)

Чтобы подключить папку с промптами, открываем настройки плагина и указываем путь к нашей папке:

![](img/20250110181730.png)

Закрываем настройки, вводим в чате "/" и видим что наш промпт появился в списке:

![](img/20250110182104.png)

Нажимаем Enter, видим что в поле ввода скопировался весь текст промпта. Дописываем параметры и отправляем:

![](img/20250110182919.png)

Видим что ChatGPT не смог распознать параметр "Сложность" который мы ему передали. Ещё бывает что он не находит характеристику персонажа. Так бывает когда запрос оказывается слишком сложным:

![](img/20250110183133.png)

Чтобы это исправить, откроем настройки плагина. В нём есть две настройки:
- Temperature — в этом случае её можно уменьшить до нуля, поскольку нам требуется точный ответ
- Token limit — её можно увеличить, чтобы разрешить модели использовать больше своих мозгов. Но учтите, что чем больше токенов — тем дороже запрос

![](img/20250110183403.png)

Нажмём кнопку для получения нового ответа:

![](img/20250110183547.png)

Теперь модель всё поняла правильно:

![](img/20250110183829.png)




# Супер-промпты

Заведём ещё один промпт, который будет придумывать художественную часть проверок, генерируя описание ситуации, в которую попал персонаж, действия игрока, и результат:

![](img/20250110184451.png)

Проверяем, работает:

![](img/20250110185523.png)

А теперь соединим механику с описанием! Создаём новый промпт, в котором ссылаемся на два других промпта:

![](img/20250110190647.png)

Ну круто же?




# Но вы не обольщайтесь

К сожалению всё это работат совсем не идеально.

Во-первых, нейронка может начать врать и придумывать там, где нужен точный ответ. В нашем примере он иногда всё-таки путает входные параметры, неверно считает шансы успеха, выводит ответ не полностью, или не соблюдает формат. Справиться с этим можно двумя способами:
1. Делать мелкие промпты, и экспериментировать с самим текстом, чтобы получать стабильные ответы на разных данных, а затем объединять в супер-промпт.
2. Подтюнить Temperature и Token limit под конкретную задачу. К сожалению, изменить эти параметры можно только для всего плагина (а было бы удобно иметь возможность указывать их в тексте промпта), так что придётся выбрать средние значения подходящие для ваших задач, либо постоянно лазить в опции. Впрочем, для разных баз знаний можно завести разные хранилища, и настроить в них плагины отдельно.

Во-вторых, модель придумывает довольно скудные сюжеты: в двадцати генерациях Сурикат либо танцевал, либо рассказывал анекдоты на деревенской площади. Но чату можно помочь, если сообщить ему больше деталей.



# [4. Локальные модели](4.%20Локальные%20модели.md)

